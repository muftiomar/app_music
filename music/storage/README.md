# ğŸ—„ï¸ Storage & HDFS

## ğŸ¯ Objectif
Architecturer et gÃ©rer le stockage des donnÃ©es musicales dans un DataLake HDFS.

## ğŸ“‹ TÃ¢ches principales
- [ ] Configuration HDFS et Hadoop
- [ ] StratÃ©gie de partitioning des donnÃ©es
- [ ] SchÃ©mas Hive pour requÃªtes SQL
- [ ] Gestion des mÃ©tadonnÃ©es

## ğŸ› ï¸ Technologies
- HDFS/Hadoop
- Apache Hive
- Format Parquet/Delta Lake
- Spark SQL

## ğŸ“ Structure des fichiers
```
storage/
â”œâ”€â”€ hdfs_config/
â”‚   â”œâ”€â”€ core-site.xml          # Configuration HDFS
â”‚   â””â”€â”€ hdfs-site.xml          # ParamÃ¨tres HDFS
â”œâ”€â”€ hive_schemas.sql           # DÃ©finition des tables Hive
â”œâ”€â”€ partitioning_strategy.py   # Logique de partitioning
â”œâ”€â”€ data_ingestion.py          # Scripts d'ingestion batch
â””â”€â”€ metadata_manager.py        # Gestion des mÃ©tadonnÃ©es
```

## ğŸš€ Quick Start
```bash
cd storage/
# DÃ©marrer HDFS
./start-hdfs.sh
# CrÃ©er les tables Hive
hive -f hive_schemas.sql
```

## ğŸ“Š Structure des donnÃ©es
```
/music/
â”œâ”€â”€ raw/                       # DonnÃ©es brutes from Kafka
â”‚   â”œâ”€â”€ music_events/          # Ã‰vÃ©nements d'Ã©coute
â”‚   â”œâ”€â”€ user_interactions/     # Likes, partages, playlists
â”‚   â””â”€â”€ system_metrics/        # MÃ©triques de performance
â”œâ”€â”€ processed/                 # DonnÃ©es nettoyÃ©es et agrÃ©gÃ©es
â”‚   â”œâ”€â”€ daily_top_artists/     # Top artistes par jour
â”‚   â”œâ”€â”€ hourly_genre_stats/    # Stats genres par heure
â”‚   â”œâ”€â”€ country_platform_stats/ # Stats par pays/plateforme
â”‚   â””â”€â”€ user_engagement/       # MÃ©triques d'engagement
â””â”€â”€ analytics/                 # Analyses avancÃ©es
    â”œâ”€â”€ dashboard/             # DonnÃ©es pour dashboard
    â”œâ”€â”€ reports/               # Rapports gÃ©nÃ©rÃ©s
    â””â”€â”€ ml_features/           # Features pour ML
```

## ğŸš€ Installation et utilisation

### DÃ©marrage rapide
```bash
cd storage/
chmod +x start_storage.sh
./start_storage.sh
```

### Installation manuelle
```bash
cd storage/
pip install -r requirements.txt
```

## ğŸ“‹ FonctionnalitÃ©s implÃ©mentÃ©es

### âœ… Infrastructure HDFS
- Configuration automatique des rÃ©pertoires
- IntÃ©gration avec Docker Hadoop existant
- Gestion des mÃ©tadonnÃ©es et partitioning
- Support Parquet avec compression Snappy

### âœ… SchÃ©mas de donnÃ©es
- **Tables brutes** : music_events, user_interactions, system_metrics
- **Tables agrÃ©gÃ©es** : daily_top_artists, hourly_genre_stats, country_platform_stats
- **Partitioning** : Par annÃ©e/mois/jour pour optimiser les requÃªtes
- **Format** : Parquet pour performance optimale

### âœ… Gestionnaire HDFS
- Classe `HDFSManager` pour toutes les opÃ©rations
- Mode simulation si HDFS indisponible
- Lecture/Ã©criture Parquet automatique
- Gestion des erreurs robuste

### âœ… Scripts d'ingestion
- GÃ©nÃ©rateur de donnÃ©es d'exemple rÃ©alistes
- Tests complets du pipeline
- Validation de l'intÃ©gritÃ© des donnÃ©es
- Support batch et streaming

## ğŸ› ï¸ Utilisation dÃ©taillÃ©e

### 1. DÃ©marrer l'infrastructure
```bash
./start_storage.sh
# Choisir option 1: DÃ©marrer Hadoop
```

### 2. CrÃ©er la structure HDFS
```bash
./start_storage.sh
# Choisir option 2: CrÃ©er structure HDFS
```

### 3. Initialiser les tables Hive
```bash
./start_storage.sh
# Choisir option 3: CrÃ©er tables Hive
```

### 4. Tester l'ingestion
```bash
./start_storage.sh
# Choisir option 4: Test d'ingestion
```

### Utilisation programmatique
```python
from hdfs_manager import HDFSManager
import pandas as pd

# Initialiser le gestionnaire
hdfs = HDFSManager()

# CrÃ©er la structure
hdfs.create_directory_structure()

# Ã‰crire des donnÃ©es
data = pd.DataFrame([...])
hdfs.write_parquet_data(data, "music_events")

# Lire des donnÃ©es
events = hdfs.read_parquet_data("music_events")

# Informations sur les tables
info = hdfs.get_table_info("music_events")
```

## ğŸ“Š SchÃ©mas de donnÃ©es

### Ã‰vÃ©nements musicaux
```sql
CREATE TABLE music_events (
    event_id STRING,
    user_id STRING,
    track_id STRING,
    artist STRING,
    title STRING,
    genre STRING,
    event_type STRING,  -- play, pause, skip, like
    timestamp TIMESTAMP,
    country STRING,
    platform STRING     -- Spotify, Apple Music, etc.
)
PARTITIONED BY (year INT, month INT, day INT)
```

### Interactions utilisateur
```sql
CREATE TABLE user_interactions (
    interaction_id STRING,
    user_id STRING,
    track_id STRING,
    interaction_type STRING,  -- like, share, playlist
    timestamp TIMESTAMP,
    metadata MAP<STRING, STRING>
)
PARTITIONED BY (year INT, month INT, day INT)
```

## ğŸ”§ Configuration

### Variables d'environnement
```bash
HDFS_HOST=localhost
HDFS_PORT=9000
HIVE_HOST=localhost
HIVE_PORT=10000
```

### Chemins HDFS configurÃ©s
- **DonnÃ©es brutes** : `/music/raw/`
- **DonnÃ©es traitÃ©es** : `/music/processed/`
- **Analytics** : `/music/analytics/`
- **Temporaire** : `/music/temp/`

## ğŸ› DÃ©pannage

### HDFS non disponible
Le systÃ¨me fonctionne en mode simulation :
```bash
./start_storage.sh
# Option 5: GÃ©nÃ©rer donnÃ©es d'exemple
```

### ProblÃ¨mes Docker
```bash
# VÃ©rifier le statut
docker ps | grep hadoop

# RedÃ©marrer si nÃ©cessaire
docker-compose restart namenode datanode
```

### Erreurs de dÃ©pendances
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### VÃ©rifier la connectivitÃ© HDFS
```bash
# Interface web Namenode
curl http://localhost:9870

# Via le script
./start_storage.sh  # Option 6
```

## ğŸ“ˆ Performance et optimisation

### Partitioning
- **Par date** : Optimise les requÃªtes temporelles
- **Format Parquet** : Compression et lecture rapide
- **Compression Snappy** : Ã‰quilibre taille/vitesse

### RÃ©tention des donnÃ©es
- **DonnÃ©es brutes** : 90 jours
- **DonnÃ©es traitÃ©es** : 1 an
- **Analytics** : 2 ans

## ğŸ”— IntÃ©gration avec les autres composants

### Avec le Producer (Personne 1)
```python
# Le producer utilise ces schÃ©mas pour Kafka
from storage.hdfs_config import HDFS_PATHS
```

### Avec le Consumer (Personne 2)
```python
# Le consumer Ã©crit directement en HDFS
hdfs.write_parquet_data(kafka_data, "music_events")
```

### Avec Analytics (Personne 4)
```python
# Analytics lit depuis HDFS
events = hdfs.read_parquet_data("music_events", start_date, end_date)
```
